{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing import image, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 2)\n"
     ]
    }
   ],
   "source": [
    "pd_dataset = pd.read_csv(\"./Flickr8k_text/flickr_8k_train_dataset.txt\", delimiter='\\t')\n",
    "ds = pd_dataset.values\n",
    "print ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for ix in range(ds.shape[0]):\n",
    "    sentences.append(ds[ix, 1])\n",
    "    \n",
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [i.split() for i in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = []\n",
    "for i in words:\n",
    "    unique.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8253\n"
     ]
    }
   ],
   "source": [
    "unique = list(set(unique))\n",
    "print len(unique)\n",
    "\n",
    "vocab_size = len(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization\n",
    "word_2_indices = {val:index for index, val in enumerate(unique)}\n",
    "indices_2_word = {index:val for index, val in enumerate(unique)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_indices['UNK'] = 0\n",
    "word_2_indices['raining'] = 8253\n",
    "\n",
    "indices_2_word[0] = 'UNK'\n",
    "indices_2_word[8253] = 'raining'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4011\n",
      "<start>\n",
      "8051\n",
      "<end>\n"
     ]
    }
   ],
   "source": [
    "print word_2_indices['<start>']\n",
    "print indices_2_word[4011]\n",
    "print word_2_indices['<end>']\n",
    "print indices_2_word[8051]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8254\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_2_indices.keys())\n",
    "print vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "for i in sentences:\n",
    "    i = i.split()\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "print max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000,)\n",
      "(30000,)\n"
     ]
    }
   ],
   "source": [
    "padded_sequences, subsequent_words = [], []\n",
    "\n",
    "for ix in range(ds.shape[0]):\n",
    "    partial_seqs = []\n",
    "    next_words = []\n",
    "    text = ds[ix, 1].split()\n",
    "    text = [word_2_indices[i] for i in text]\n",
    "    for i in range(1, len(text)):\n",
    "        partial_seqs.append(text[:i])\n",
    "        next_words.append(text[i])\n",
    "    padded_partial_seqs = sequence.pad_sequences(partial_seqs, max_len, padding='post')\n",
    "\n",
    "    next_words_1hot = np.zeros([len(next_words), vocab_size], dtype=np.bool)\n",
    "    \n",
    "    #Vectorization\n",
    "    for i,next_word in enumerate(next_words):\n",
    "        next_words_1hot[i, next_word] = 1\n",
    "        \n",
    "    padded_sequences.append(padded_partial_seqs)\n",
    "    subsequent_words.append(next_words_1hot)\n",
    "    \n",
    "padded_sequences = np.asarray(padded_sequences)\n",
    "subsequent_words = np.asarray(subsequent_words)\n",
    "\n",
    "print padded_sequences.shape\n",
    "print subsequent_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4011    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [4011  109    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [4011  109 4879    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [4011  109 4879 4331    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [4011  109 4879 4331 4426    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [4011  109 4879 4331 4426 4980    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [4011  109 4879 4331 4426 4980 2192    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [4011  109 4879 4331 4426 4980 2192 3196    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [4011  109 4879 4331 4426 4980 2192 3196 6915    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [4011  109 4879 4331 4426 4980 2192 3196 6915 4331    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [4011  109 4879 4331 4426 4980 2192 3196 6915 4331 4433    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [4011  109 4879 4331 4426 4980 2192 3196 6915 4331 4433 2434    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [4011  109 4879 4331 4426 4980 2192 3196 6915 4331 4433 2434 6432    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [4011  109 4879 4331 4426 4980 2192 3196 6915 4331 4433 2434 6432  324\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print padded_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
      "\n",
      "<start> A UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
      "\n",
      "<start> A black UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
      "\n",
      "<start> A black dog UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
      "\n",
      "<start> A black dog is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
      "\n",
      "<start> A black dog is running UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
      "\n",
      "<start> A black dog is running after UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
      "\n",
      "<start> A black dog is running after a UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
      "\n",
      "<start> A black dog is running after a white UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
      "\n",
      "<start> A black dog is running after a white dog UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
      "\n",
      "<start> A black dog is running after a white dog in UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
      "\n",
      "<start> A black dog is running after a white dog in the UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
      "\n",
      "<start> A black dog is running after a white dog in the snow UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
      "\n",
      "<start> A black dog is running after a white dog in the snow . UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
      "\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "for ix in range(len(padded_sequences[0])):\n",
    "    for iy in range(max_len):\n",
    "        print indices_2_word[padded_sequences[0][ix][iy]],\n",
    "    print \"\\n\"\n",
    "\n",
    "print len(padded_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_images = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25493, 40)\n",
      "(25493, 8254)\n"
     ]
    }
   ],
   "source": [
    "captions = np.zeros([0, max_len])\n",
    "next_words = np.zeros([0, vocab_size])\n",
    "\n",
    "for ix in range(num_of_imaages):#img_to_padded_seqs.shape[0]):\n",
    "    captions = np.concatenate([captions, padded_sequences[ix]])\n",
    "    next_words = np.concatenate([next_words, subsequent_words[ix]])\n",
    "\n",
    "np.save(\"./captions.npy\", captions)\n",
    "np.save(\"./next_words.npy\", next_words)\n",
    "\n",
    "print captions.shape\n",
    "print next_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./train_encoded_images.p', 'rb') as f:\n",
    "        encoded_images = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 2048)\n"
     ]
    }
   ],
   "source": [
    "imgs = []\n",
    "\n",
    "for ix in range(ds.shape[0]):\n",
    "    if ds[ix, 0] in encoded_images.keys():\n",
    "        imgs.append(list(encoded_images[ds[ix, 0]]))\n",
    "\n",
    "imgs = np.asarray(imgs)\n",
    "print imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25493, 2048)\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "\n",
    "for ix in range(num_of_images):\n",
    "    for iy in range(padded_sequences[ix].shape[0]):\n",
    "        images.append(imgs[ix])\n",
    "        \n",
    "images = np.asarray(images)\n",
    "\n",
    "np.save(\"./images.npy\", images)\n",
    "\n",
    "print images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25493\n"
     ]
    }
   ],
   "source": [
    "image_names = []\n",
    "\n",
    "for ix in range(num_of_images):\n",
    "    for iy in range(padded_sequences[ix].shape[0]):\n",
    "        image_names.append(ds[ix, 0])\n",
    "        \n",
    "image_names = np.asarray(image_names)\n",
    "\n",
    "np.save(\"./image_names.npy\", image_names)\n",
    "\n",
    "print len(image_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
